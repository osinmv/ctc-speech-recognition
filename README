# Notes

Model: SimpleRNNModel
Average loss over test dataset is: -1.3170893368561094
Produces zeros, cause they are the most frequent

Learned that I should apply log_softmax for my model output, cause CTC loss expects log probabilities for input

Model: SimpleTransformer
Average loss over test dataset is: -1.3968925688688347
Producing spaces and nothings

# May 1st 2025

Learned that CTC expects actual input lengths and target lengths
 - Was passing padded lengths causing the model output 0s all the time

Learned that model actually converges faster if you decrease the size of output dimention to nessessary tokens

First semi-success, trained an LSTM model to recognize speech
 - Producing almost coherent text

## Example
```
Original: BUT THE MEMORY OF THEIR EXPLOITS HAS PASSED AWAY OWING TO THE LAPSE OF TIME AND THE EXTINCTION OF THE ACTORS


Recognised: be the menmey of their yxpoite hat past ta wa oing to the wasitime and the xtantion op e eturs
```



## May 2nd 2025

Trained a lsightly bigger LSTM model
After a quick talk with GPT decided to implement language model to assist speech recognition in beam search by weighting in about spelling


## Next steps

 - Train Transformer Model
 - Train using phonemes instead of letters of english alphabet
